{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'surface_langchain_deprecation_warnings' from 'langchain_core._api.deprecation' (/home/huncho/Workspace/final_project_qna/.venv/lib/python3.10/site-packages/langchain_core/_api/deprecation.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvectorstores\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Chroma\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membeddings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpenAIEmbeddings\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext_splitter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RecursiveCharacterTextSplitter\n",
      "File \u001b[0;32m~/Workspace/final_project_qna/.venv/lib/python3.10/site-packages/langchain/__init__.py:8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mimportlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m metadata\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any, Optional\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdeprecation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m surface_langchain_deprecation_warnings\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     11\u001b[0m     __version__ \u001b[38;5;241m=\u001b[39m metadata\u001b[38;5;241m.\u001b[39mversion(__package__)\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'surface_langchain_deprecation_warnings' from 'langchain_core._api.deprecation' (/home/huncho/Workspace/final_project_qna/.venv/lib/python3.10/site-packages/langchain_core/_api/deprecation.py)"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.llms import OpenAI \n",
    "from langchain.chains import VectorDBQA\n",
    "from langchain.document_loaders import TextLoader ,PyPDFLoader,PDFPlumberLoader\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_KEY']='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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_loader = TextLoader('state_of_union.txt')\n",
    "txt_documents = txt_loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_loader = PyPDFLoader('projects2.pdf')\n",
    "pdf_documents = pdf_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'projects2.pdf', 'page': 0}, page_content='PROJET ANNUEL, MASTER 1  \\n \\nPROJET  ANNUEL  \\n \\nNOM ET PRÉNOMS  PROJET  \\n 01 \\nAnalyse en temps réel de flux vidéo : Détection, \\nSegmentation et Comptage de véhicules   \\n \\n \\n \\n02 \\nTraduction EWE – Anglais, Anglais - EWE   \\n \\n \\n 03 \\nEntraîner votre propre mini modèle  de \\ngénération de texte tel que ChatGPT   \\n \\n \\n \\n04 \\nSystème de questions -réponses   \\n \\n \\n \\n \\nEvaluation:  \\n1. Rapport:  Soumettre u n rapport détaillant les étapes de réalisation, les difficultés , les \\nsolutions  ainsi que les contributions de chaque member.  A soumettre avant la date de \\npresentation  : tchaye59@gmail.com (10 points ) \\n2. Presentation:  Présentation des projets ( 10 points ) \\n \\n \\n \\n \\nDate de Présentation: 31 -août-24'), Document(metadata={'source': 'projects2.pdf', 'page': 1}, page_content='PROJET ANNUEL, MASTER 1  \\nLISTE DES PROJETS  \\n \\nI. PROJET 1 : ANALYSE EN  TEMPS RÉEL DE FLUX VIDÉO : DÉTECTION, \\nSEGMENTATION ET COMPTAGE DE VÉHICULES  \\n \\nObjectif :  Analyser les objets dans une vidéo filmant une voie fréquentée par des véhicules \\nsegmentés et compter le nombre de véhicules sur chaque image. Lorsque l’utilisateur clique sur un \\nvéhicule, afficher sa plaque d’immatriculation.  \\n \\nOutils:  Voici quelques outils et bibliothèques que vous pouvez utiliser pour réaliser votre projet:  \\n1. OpenCV (Open Source Computer Vision Library) : OpenCV est une bibliothèque open -\\nsource très populaire pour le traitement d’images et la vision par ordinateur. Elle propose \\ndes fonctionnalités telles que la détection d’objets, la segmentation d’images et le  suivi de \\nmouvement. Vous pouvez l’utiliser pour extraire des informations à partir de vidéos.  \\n2. YOLO (You Only Look Once) : YOLO est un modèle de détection d’objets en temps réel qui'), Document(metadata={'source': 'projects2.pdf', 'page': 1}, page_content='Outils:  Voici quelques outils et bibliothèques que vous pouvez utiliser pour réaliser votre projet:  \\n1. OpenCV (Open Source Computer Vision Library) : OpenCV est une bibliothèque open -\\nsource très populaire pour le traitement d’images et la vision par ordinateur. Elle propose \\ndes fonctionnalités telles que la détection d’objets, la segmentation d’images et le  suivi de \\nmouvement. Vous pouvez l’utiliser pour extraire des informations à partir de vidéos.  \\n2. YOLO (You Only Look Once) : YOLO est un modèle de détection d’objets en temps réel qui \\npeut être utilisé pour détecter des véhicules dans des images ou des vidéos. Il est rapide et \\nprécis.  \\n3. Mask R-CNN : Si vous avez besoin de segmenter les véhicules (c’est -à-dire les isoler du \\nreste de l’image), Mask R -CNN est un excellent choix. Il permet la segmentation \\nsémantique et peut être utilisé pour extraire des masques précis autour des véhicules.'), Document(metadata={'source': 'projects2.pdf', 'page': 1}, page_content='mouvement. Vous pouvez l’utiliser pour extraire des informations à partir de vidéos.  \\n2. YOLO (You Only Look Once) : YOLO est un modèle de détection d’objets en temps réel qui \\npeut être utilisé pour détecter des véhicules dans des images ou des vidéos. Il est rapide et \\nprécis.  \\n3. Mask R-CNN : Si vous avez besoin de segmenter les véhicules (c’est -à-dire les isoler du \\nreste de l’image), Mask R -CNN est un excellent choix. Il permet la segmentation \\nsémantique et peut être utilisé pour extraire des masques précis autour des véhicules.  \\n4. NumPy et SciPy : Ces bibliothèques Python sont essentielles pour le traitement d’images, \\nles calculs matriciels et les opérations statistiques. Vous pouvez les utiliser pour manipuler \\nles données d’image et effectuer des calculs.  \\n5. Tesseract OCR : Si vous souhaitez extraire des plaques d’immatriculation à partir d’images \\nde véhicules, Tesseract OCR est un outil open -source qui peut reconnaître du texte dans \\ndes images.'), Document(metadata={'source': 'projects2.pdf', 'page': 1}, page_content='sémantique et peut être utilisé pour extraire des masques précis autour des véhicules.  \\n4. NumPy et SciPy : Ces bibliothèques Python sont essentielles pour le traitement d’images, \\nles calculs matriciels et les opérations statistiques. Vous pouvez les utiliser pour manipuler \\nles données d’image et effectuer des calculs.  \\n5. Tesseract OCR : Si vous souhaitez extraire des plaques d’immatriculation à partir d’images \\nde véhicules, Tesseract OCR est un outil open -source qui peut reconnaître du texte dans \\ndes images.  \\n6. Flask ou FastAPI : Pour créer une interface utilisateur et gérer les interactions avec \\nl’utilisateur, vous pouvez utiliser Flask (pour une approche plus simple) ou FastAPI (pour \\nune approche plus performante).  \\n \\nÉtapes : Voici les étapes pour la réalisation de votre projet  \\n1. Collecte de données  : \\no Trouvez ou enregistrez des vidéos de voies fréquentées par des véhicules. Plus vous \\navez de données, mieux c’est pour l’entraînement de votre modèle.'), Document(metadata={'source': 'projects2.pdf', 'page': 2}, page_content='PROJET ANNUEL, MASTER 1  \\n2. Prétraitement des données  : \\no Utilisez des outils comme OpenCV pour extraire des images individuelles à partir de \\nla vidéo.  \\no Redimensionnez les images pour une taille cohérente.  \\no Étiquetez manuellement les véhicules dans les images (si vous avez des \\nannotations).  \\n3. Détection d’objets  : \\no Entraînez un modèle de détection d’objets (comme YOLO) sur vos images annotées.  \\no Utilisez le modèle entraîné pour détecter les véhicules dans les images de la vidéo.  \\n4. Segmentation d’images  : \\no Si vous souhaitez segmenter les véhicules, utilisez un modèle comme Mask R -CNN.  \\no Appliquez la segmentation aux véhicules détectés.  \\n5. Comptage de véhicules  : \\no Pour chaque image, comptez le nombre de véhicules détectés.  \\n6. Affichage des plaques d’immatriculation  : \\no Utilisez Tesseract OCR pour extraire le texte des plaques d’immatriculation.  \\no Lorsque l’utilisateur clique sur un véhicule détecté, affichez la plaque'), Document(metadata={'source': 'projects2.pdf', 'page': 2}, page_content='o Utilisez le modèle entraîné pour détecter les véhicules dans les images de la vidéo.  \\n4. Segmentation d’images  : \\no Si vous souhaitez segmenter les véhicules, utilisez un modèle comme Mask R -CNN.  \\no Appliquez la segmentation aux véhicules détectés.  \\n5. Comptage de véhicules  : \\no Pour chaque image, comptez le nombre de véhicules détectés.  \\n6. Affichage des plaques d’immatriculation  : \\no Utilisez Tesseract OCR pour extraire le texte des plaques d’immatriculation.  \\no Lorsque l’utilisateur clique sur un véhicule détecté, affichez la plaque \\nd’immatriculation correspondante.  \\n7. Interface utilisateur  : \\no Créez une interface utilisateur (par exemple, avec Flask ou FastAPI) pour afficher les \\nrésultats et gérer les interactions.  \\n8. Tests et optimisation  : \\no Testez votre modèle sur différentes vidéos.  \\no Optimisez les performances (par exemple, en ajustant les seuils de détection).  \\n9. Déploiement  : \\no Déployez votre application pour qu’elle fonctionne en temps réel.'), Document(metadata={'source': 'projects2.pdf', 'page': 2}, page_content='o Lorsque l’utilisateur clique sur un véhicule détecté, affichez la plaque \\nd’immatriculation correspondante.  \\n7. Interface utilisateur  : \\no Créez une interface utilisateur (par exemple, avec Flask ou FastAPI) pour afficher les \\nrésultats et gérer les interactions.  \\n8. Tests et optimisation  : \\no Testez votre modèle sur différentes vidéos.  \\no Optimisez les performances (par exemple, en ajustant les seuils de détection).  \\n9. Déploiement  : \\no Déployez votre application pour qu’elle fonctionne en temps réel.'), Document(metadata={'source': 'projects2.pdf', 'page': 3}, page_content='PROJET ANNUEL, MASTER 1  \\n \\nII. PROJET 2 : TRADUCTION EWE – ANGLAIS, ANGLAIS - EWE  \\n \\nObjectif :  Créer un modèle de traduction bidirectionnelle entre l’EWE et l’anglais en utilisant le \\njeu de données fourni  sur le lien suivant :  EWE-English Bilingual Pairs . \\nEtapes:  \\n1. Prétraitement des données:  \\no Nettoyez et préparez vos données en supprimant les caractères spéciaux, les \\ndoublons et en normalisant le texte.  \\no Divisez vos données en ensembles d’entraînement, de validation et de test.  \\n2. Choix du modèle :  \\no Creer votre propore model Transformateurs  (comme BERT ou GPT).  Les resaux \\nexistent et preentrainer ne sont pas autoriser.  \\no Assurez-vous que votre modèle est capable de gérer les deux directions de \\ntraduction (EWE vers anglais et anglais vers EWE).  \\n3. Entraînement du modèle :  \\no Alimentez votre modèle avec les paires de phrases EWE -anglais (EWE-English \\nBilingual Pairs ). \\no Expérimentez avec différentes architectures et hyperparamètres pour obtenir de'), Document(metadata={'source': 'projects2.pdf', 'page': 3}, page_content='o Divisez vos données en ensembles d’entraînement, de validation et de test.  \\n2. Choix du modèle :  \\no Creer votre propore model Transformateurs  (comme BERT ou GPT).  Les resaux \\nexistent et preentrainer ne sont pas autoriser.  \\no Assurez-vous que votre modèle est capable de gérer les deux directions de \\ntraduction (EWE vers anglais et anglais vers EWE).  \\n3. Entraînement du modèle :  \\no Alimentez votre modèle avec les paires de phrases EWE -anglais (EWE-English \\nBilingual Pairs ). \\no Expérimentez avec différentes architectures et hyperparamètres pour obtenir de \\nbons résultats.  \\n4. Évaluation du modèle :  \\no Évaluez les performances de votre modèle sur l’ensemble de test en utilisant des \\nmétriques telles que le BLEU score (qui mesure la qualité de la traduction).  \\no Réalisez des ajustements si nécessaire.  \\n5. Inference et déploiement :  \\no Utilisez votre modèle entraîné pour traduire des phrases de l’EWE vers l’anglais et \\nvice versa.'), Document(metadata={'source': 'projects2.pdf', 'page': 3}, page_content='3. Entraînement du modèle :  \\no Alimentez votre modèle avec les paires de phrases EWE -anglais (EWE-English \\nBilingual Pairs ). \\no Expérimentez avec différentes architectures et hyperparamètres pour obtenir de \\nbons résultats.  \\n4. Évaluation du modèle :  \\no Évaluez les performances de votre modèle sur l’ensemble de test en utilisant des \\nmétriques telles que le BLEU score (qui mesure la qualité de la traduction).  \\no Réalisez des ajustements si nécessaire.  \\n5. Inference et déploiement :  \\no Utilisez votre modèle entraîné pour traduire des phrases de l’EWE vers l’anglais et \\nvice versa.  \\no Vous pouvez déployer votre modèle en tant qu’API et l’intégrer dans une application  \\n(Web,mobile ou desktop) .'), Document(metadata={'source': 'projects2.pdf', 'page': 4}, page_content='PROJET ANNUEL, MASTER 1  \\n \\n \\nIII. PROJET 3 : ENTRAÎNER VOTRE PROPRE MINI MODÈLE DE GÉNÉRATION DE \\nTEXTE TEL QUE CHATGPT  \\n  \\nObjectif :  Entraîner un mini assistant virtuel, similaire à ChatGPT (Mini ChatGPT), capable de \\nrépondre de manière humaine à chaque question. Utilisez les données d’OpenAssistant \\ndisponibles sur les liens suivants : \\nOpenAssistant/oasst1 (https://huggingface.co/datasets/OpenAssistant/oasst1 ) et \\nOpenAssistant/oasst2 (https://huggingface.co/datasets/OpenAssistant/oasst2 ). Concentrez -vous \\nuniquement sur la langue française.  \\nEtapes:  \\n1. Collecte de données :  Rassemblez un ensemble de données en français pour \\nl’entraînement. Vous pouvez utiliser les données d’OpenAssistant (OpenAssistant/oasst1 , \\nOpenAssistant/oasst 2). \\n2. Prétraitement des données :  Nettoyez et prétraitez vos données. Cela peut inclure  la \\nsuppression des balises HTML, la normalisation des textes, la tokenisation, etc.'), Document(metadata={'source': 'projects2.pdf', 'page': 4}, page_content='OpenAssistant/oasst1 (https://huggingface.co/datasets/OpenAssistant/oasst1 ) et \\nOpenAssistant/oasst2 (https://huggingface.co/datasets/OpenAssistant/oasst2 ). Concentrez -vous \\nuniquement sur la langue française.  \\nEtapes:  \\n1. Collecte de données :  Rassemblez un ensemble de données en français pour \\nl’entraînement. Vous pouvez utiliser les données d’OpenAssistant (OpenAssistant/oasst1 , \\nOpenAssistant/oasst 2). \\n2. Prétraitement des données :  Nettoyez et prétraitez vos données. Cela peut inclure  la \\nsuppression des balises HTML, la normalisation des textes, la tokenisation, etc.  \\n3. Créez votre modèle de transformateurs :  Concevez votre propre architecture de modèle \\nde transformateurs. Vous pouvez utiliser des bibliothèques telles que Hugging Face \\nTransformers .... \\n4. Entraînement du modèle :  Entraînez votre modèle sur les données prétraitées. Utilisez des \\ntechniques telles que l’apprentissage supervisé.'), Document(metadata={'source': 'projects2.pdf', 'page': 4}, page_content='OpenAssistant/oasst 2). \\n2. Prétraitement des données :  Nettoyez et prétraitez vos données. Cela peut inclure  la \\nsuppression des balises HTML, la normalisation des textes, la tokenisation, etc.  \\n3. Créez votre modèle de transformateurs :  Concevez votre propre architecture de modèle \\nde transformateurs. Vous pouvez utiliser des bibliothèques telles que Hugging Face \\nTransformers .... \\n4. Entraînement du modèle :  Entraînez votre modèle sur les données prétraitées. Utilisez des \\ntechniques telles que l’apprentissage supervisé.  \\n5. Déploiement :  Une fois que votre modèle est satisfaisant, déployez -le pour qu’il puisse \\nrépondre aux questions de manière humaine.'), Document(metadata={'source': 'projects2.pdf', 'page': 5}, page_content='PROJET ANNUEL, MASTER 1  \\nIV. PROJET 4 : SYSTÈME DE QUESTIONS -RÉPONSES : DÉVELOPPER UN \\nSYSTÈME CAPABLE DE RÉPONDRE À DES QUESTIONS FORMULÉES EN \\nLANGAGE NATUREL , BASÉ SUR UN ENSEMBLE DE DOCUMENTS DONNÉS . \\n \\nObjectif : Développer un système  intelligent capable de prendre des documents en entrée \\net de répondre à chaque question posée par l’utilisateur concernant ces documents. Utilisez \\ndes outils tels que Langchain.  \\n \\nÉtapes   \\n1. Collecte de données  : Rassemblez  un ensemble de documents pertinents qui serviront de \\nbase pour répondre aux questions. Ces documents peuvent être des articles, des livres, \\ndes rapports, etc.  \\n2. Prétraitement des données  : Nettoyez et formatez les documents pour faciliter leur \\nanalyse. Cela peut inclure la suppression des stop words, la lemmatisation, etc.  \\n3. Indexation des documents  : Créez un index des documents pour faciliter la recherche \\nd’informations pertinentes lors de la réponse aux questions.'), Document(metadata={'source': 'projects2.pdf', 'page': 5}, page_content='des outils tels que Langchain.  \\n \\nÉtapes   \\n1. Collecte de données  : Rassemblez  un ensemble de documents pertinents qui serviront de \\nbase pour répondre aux questions. Ces documents peuvent être des articles, des livres, \\ndes rapports, etc.  \\n2. Prétraitement des données  : Nettoyez et formatez les documents pour faciliter leur \\nanalyse. Cela peut inclure la suppression des stop words, la lemmatisation, etc.  \\n3. Indexation des documents  : Créez un index des documents pour faciliter la recherche \\nd’informations pertinentes lors de la réponse aux questions.  \\n4. Développement du modèle de questions -réponses  : Utilisez des techniques \\nd’apprentissage automatique et de traitement du langage naturel pour développer un \\nmodèle capable de comprendre les questions en langage naturel et de rechercher des \\nréponses dans l’ensemble de documents.  \\n5. Intégration de l’outil Langchain  : Intégrez l’outil Langchain pour améliorer les capacités de \\ntraitement du langage naturel du système.'), Document(metadata={'source': 'projects2.pdf', 'page': 5}, page_content='3. Indexation des documents  : Créez un index des documents pour faciliter la recherche \\nd’informations pertinentes lors de la réponse aux questions.  \\n4. Développement du modèle de questions -réponses  : Utilisez des techniques \\nd’apprentissage automatique et de traitement du langage naturel pour développer un \\nmodèle capable de comprendre les questions en langage naturel et de rechercher des \\nréponses dans l’ensemble de documents.  \\n5. Intégration de l’outil Langchain  : Intégrez l’outil Langchain pour améliorer les capacités de \\ntraitement du langage naturel du système.  \\n6. Test et évaluation  : Testez le système avec un ensemble de questions pour évaluer sa \\nperformance. Faites les ajustements nécessaires pour améliorer la précision des réponses.  \\n7. Déploiement  : Une fois que le système fonctionne de manière satisfaisante, déployez -le \\npour qu’il soit utilisé par les utilisateurs finaux.')]\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=600)\n",
    "texts = text_splitter.split_documents(pdf_documents)\n",
    "print(texts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Could not import chromadb python package. Please install it with `pip install chromadb`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/Workspace/final_project_qna/.venv/lib/python3.10/site-packages/langchain_community/vectorstores/chroma.py:83\u001b[0m, in \u001b[0;36mChroma.__init__\u001b[0;34m(self, collection_name, embedding_function, persist_directory, client_settings, collection_metadata, client, relevance_score_fn)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 83\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mchromadb\u001b[39;00m\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mchromadb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'chromadb'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m vectordb \u001b[38;5;241m=\u001b[39m \u001b[43mChroma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Workspace/final_project_qna/.venv/lib/python3.10/site-packages/langchain_community/vectorstores/chroma.py:878\u001b[0m, in \u001b[0;36mChroma.from_documents\u001b[0;34m(cls, documents, embedding, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[0m\n\u001b[1;32m    876\u001b[0m texts \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mpage_content \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[1;32m    877\u001b[0m metadatas \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[0;32m--> 878\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_texts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtexts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollection_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpersist_directory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpersist_directory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient_settings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    887\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcollection_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollection_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    888\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Workspace/final_project_qna/.venv/lib/python3.10/site-packages/langchain_community/vectorstores/chroma.py:814\u001b[0m, in \u001b[0;36mChroma.from_texts\u001b[0;34m(cls, texts, embedding, metadatas, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[0m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_texts\u001b[39m(\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;28mcls\u001b[39m: Type[Chroma],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    793\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    794\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Chroma:\n\u001b[1;32m    795\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create a Chroma vectorstore from a raw documents.\u001b[39;00m\n\u001b[1;32m    796\u001b[0m \n\u001b[1;32m    797\u001b[0m \u001b[38;5;124;03m    If a persist_directory is specified, the collection will be persisted there.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;124;03m        Chroma: Chroma vectorstore.\u001b[39;00m\n\u001b[1;32m    813\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 814\u001b[0m     chroma_collection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    815\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollection_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    816\u001b[0m \u001b[43m        \u001b[49m\u001b[43membedding_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    817\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpersist_directory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpersist_directory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    818\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclient_settings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    819\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    820\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcollection_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollection_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    821\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    822\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    823\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    824\u001b[0m         ids \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mstr\u001b[39m(uuid\u001b[38;5;241m.\u001b[39muuid4()) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m texts]\n",
      "File \u001b[0;32m~/Workspace/final_project_qna/.venv/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:205\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.finalize.<locals>.warn_if_direct_instance\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    203\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    204\u001b[0m     emit_warning()\n\u001b[0;32m--> 205\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Workspace/final_project_qna/.venv/lib/python3.10/site-packages/langchain_community/vectorstores/chroma.py:86\u001b[0m, in \u001b[0;36mChroma.__init__\u001b[0;34m(self, collection_name, embedding_function, persist_directory, client_settings, collection_metadata, client, relevance_score_fn)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mchromadb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m---> 86\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     87\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not import chromadb python package. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     88\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease install it with `pip install chromadb`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     89\u001b[0m     )\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m client \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client_settings \u001b[38;5;241m=\u001b[39m client_settings\n",
      "\u001b[0;31mImportError\u001b[0m: Could not import chromadb python package. Please install it with `pip install chromadb`."
     ]
    }
   ],
   "source": [
    "vectordb = Chroma.from_documents(texts, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = VectorDBQA.from_chain_type(llm=OpenAI(), chain_type=\"stuff\", vectorstore=vectordb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What did the president say about Ketanji Brown Jackson\"\n",
    "qa.run(query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
